{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb71b74",
   "metadata": {},
   "source": [
    "# LunarLander-v2 solver\n",
    "## 1. Functions and classes\n",
    "Import packages, network class setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7160e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as sp\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 테스트 결과 mps가 cpu보다 더 느림 왜 인지 모르겠슴. 그래서 주석으로 막음\n",
    "# # torch.device('mps') 이렇게 세팅 안하고 device = 'mps' 해도 되는 것 확인\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "#%% Policy network\n",
    "class QNet(nn.Module):\n",
    "    # Policy Network\n",
    "    def __init__(self, n_states, n_actions, n_hidden=64):\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_states, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_actions)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "#%% dqn    \n",
    "class DQN():\n",
    "    def __init__(self, n_states, n_actions, batch_size=64, lr=1e-4, gamma=0.99, mem_size=int(1e5), learn_step=5, tau=1e-3):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learn_step = learn_step\n",
    "        self.tau = tau\n",
    "\n",
    "        # model\n",
    "        self.net_eval = QNet(n_states, n_actions).to(device)\n",
    "        self.net_target = QNet(n_states, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net_eval.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # memory\n",
    "        self.memory = ReplayBuffer(n_actions, mem_size, batch_size)\n",
    "        self.counter = 0    # update cycle counter\n",
    "\n",
    "    def getAction(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        self.net_eval.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.net_eval(state)\n",
    "        self.net_eval.train()\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(np.arange(self.n_actions))\n",
    "        else:\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save2memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.counter += 1\n",
    "        if self.counter % self.learn_step == 0:\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        q_target = self.net_target(next_states).detach().max(axis=1)[0].unsqueeze(1)\n",
    "        y_j = rewards + self.gamma * q_target * (1 - dones)          # target, if terminal then y_j = rewards\n",
    "        q_eval = self.net_eval(states).gather(1, actions)\n",
    "\n",
    "        # loss backprop\n",
    "        loss = self.criterion(q_eval, y_j)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # soft update target network\n",
    "        self.softUpdate()\n",
    "\n",
    "    def softUpdate(self):\n",
    "        for eval_param, target_param in zip(self.net_eval.parameters(), self.net_target.parameters()):\n",
    "            target_param.data.copy_(self.tau*eval_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, n_actions, memory_size, batch_size):\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen = memory_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1e3e9",
   "metadata": {},
   "source": [
    "Traning and Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f40bd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=2000, max_steps=1000, eps_start=1.0, eps_end=0.08, eps_decay=0.995, target=250, chkpt=False):\n",
    "    score_hist = []\n",
    "    epsilon = eps_start\n",
    "\n",
    "    bar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
    "    # bar_format = '{l_bar}{bar:10}{r_bar}'\n",
    "    pbar = trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
    "    for idx_epi in pbar:\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        for idx_step in range(max_steps):\n",
    "            action = agent.getAction(state, epsilon)\n",
    "            next_state, reward, done, _, info = env.step(action)\n",
    "            agent.save2memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        score_hist.append(score)\n",
    "        score_avg = np.mean(score_hist[-100:])\n",
    "        epsilon = max(eps_end, epsilon*eps_decay)\n",
    "\n",
    "        pbar.set_postfix_str(f\"Score: {score: 7.2f}, 100 score avg: {score_avg: 7.2f}, epsilon: {epsilon: 7.2f}\")\n",
    "        pbar.update(0)\n",
    "\n",
    "        # if (idx_epi+1) % 100 == 0:\n",
    "        #     print(\" \")\n",
    "        #     sleep(0.1)\n",
    "\n",
    "        # Early stop\n",
    "        if len(score_hist) >= 100:\n",
    "            if score_avg >= target:\n",
    "                break\n",
    "\n",
    "    if (idx_epi+1) < n_episodes:\n",
    "        print(\"\\nTarget Reached!\")\n",
    "    else:\n",
    "        print(\"\\nDone!\")\n",
    "        \n",
    "    if chkpt:\n",
    "        torch.save(agent.net_eval.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    return score_hist\n",
    "\n",
    "#%% Test Lunar Lander\n",
    "def testLander(env, agent, loop=3):\n",
    "    for i in range(loop):\n",
    "        state, info = env.reset()\n",
    "        for idx_step in range(500):\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            env.render()\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    \n",
    "def plotScore(scores):\n",
    "    plt.figure()\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Score History\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94829961",
   "metadata": {},
   "source": [
    "## 2. Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57a9abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPISODES = 10000\n",
    "TARGET_SCORE = 275      # early training stop at avg score of last 100 episodes\n",
    "GAMMA = 0.99            # discount factor\n",
    "MEMORY_SIZE = 10000     # max memory buffer size\n",
    "LEARN_STEP = 10          # how often to learn\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "SAVE_CHKPT = False      # save trained network .pth file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02799e76",
   "metadata": {},
   "source": [
    "## 3. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "407052f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          |    0/10000 [  00:00<      ?, ?ep/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|#5        | 1563/10000 [  10:38<  42:51,  3.28ep/s, Score:    7.14, 100 score avg:  181.35]"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "agent = DQN(\n",
    "    n_states = num_states,\n",
    "    n_actions = num_actions,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    lr = LR,\n",
    "    gamma = GAMMA,\n",
    "    mem_size = MEMORY_SIZE,\n",
    "    learn_step = LEARN_STEP,\n",
    "    tau = TAU,\n",
    "    )\n",
    "score_hist = train(env, agent, n_episodes=EPISODES, target=TARGET_SCORE, chkpt=SAVE_CHKPT)\n",
    "plotScore(score_hist)\n",
    "\n",
    "# if str(device) == \"cuda\":\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "if str(device) == \"mps\":\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb59b1",
   "metadata": {},
   "source": [
    "## 4. Test the LunarLander!\n",
    "Run code below to test trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72b48127",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "testLander(env, agent, loop=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4be6b2",
   "metadata": {},
   "source": [
    "Run code below to save tested results into a gif!   \n",
    "Saved gifs will appear in `gifs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c03247c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving gif...Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def TextOnImg(img, score):\n",
    "    img = Image.fromarray(img)\n",
    "    # font = ImageFont.truetype('/Library/Fonts/arial.ttf', 18)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # draw.text((20, 20), f\"Score={score: .2f}\", font=font, fill=(255, 255, 255))\n",
    "    draw.text((20, 20), f\"Score={score: .2f}\", fill=(255, 255, 255))\n",
    "\n",
    "    return np.array(img)\n",
    "\n",
    "def save_frames_as_gif(frames, filename, path=\"gifs/\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    print(\"Saving gif...\", end=\"\")\n",
    "    imageio.mimsave(path + filename + \".gif\", frames, duration=30)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "def gym2gif(env, agent, filename=\"gym_animation\", loop=3):\n",
    "    frames = []\n",
    "    for i in range(loop):\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        for idx_step in range(500):\n",
    "            frame = env.render()\n",
    "            frames.append(TextOnImg(frame, score))\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    save_frames_as_gif(frames, filename=filename)\n",
    "\n",
    "gym2gif(env, agent, loop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d402f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01c36111a1525a5ff407868170e7863364c687246ea4e433c1c4d9e429760133"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
