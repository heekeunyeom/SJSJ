{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb71b74",
   "metadata": {},
   "source": [
    "# LunarLander-v2 solver\n",
    "## 1. Functions and classes\n",
    "Import packages, network class setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7160e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces as sp\n",
    "from tqdm import trange\n",
    "from time import sleep\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# 테스트 결과 mps가 cpu보다 더 느림 왜 인지 모르겠슴. 그래서 주석으로 막음\n",
    "# # torch.device('mps') 이렇게 세팅 안하고 device = 'mps' 해도 되는 것 확인\n",
    "device = \"mps\" if getattr(torch,'has_mps',False) \\\n",
    "    else \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "#%% Policy network\n",
    "class QNet(nn.Module):\n",
    "    # Policy Network\n",
    "    def __init__(self, n_states, n_actions, n_hidden=64):\n",
    "        super(QNet, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_states, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_actions)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "#%% dqn    \n",
    "class DQN():\n",
    "    def __init__(self, n_states, n_actions, batch_size=64, lr=1e-4, gamma=0.99, mem_size=int(1e5), learn_step=5, tau=1e-3):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learn_step = learn_step\n",
    "        self.tau = tau\n",
    "\n",
    "        # model\n",
    "        self.net_eval = QNet(n_states, n_actions).to(device)\n",
    "        self.net_target = QNet(n_states, n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net_eval.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # memory\n",
    "        self.memory = ReplayBuffer(n_actions, mem_size, batch_size)\n",
    "        self.counter = 0    # update cycle counter\n",
    "\n",
    "    def getAction(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "\n",
    "        self.net_eval.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.net_eval(state)\n",
    "        self.net_eval.train()\n",
    "\n",
    "        # epsilon-greedy\n",
    "        if random.random() < epsilon:\n",
    "            action = random.choice(np.arange(self.n_actions))\n",
    "        else:\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        return action\n",
    "\n",
    "    def save2memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        self.counter += 1\n",
    "        if self.counter % self.learn_step == 0:\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences)\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        q_target = self.net_target(next_states).detach().max(axis=1)[0].unsqueeze(1)\n",
    "        y_j = rewards + self.gamma * q_target * (1 - dones)          # target, if terminal then y_j = rewards\n",
    "        q_eval = self.net_eval(states).gather(1, actions)\n",
    "\n",
    "        # loss backprop\n",
    "        loss = self.criterion(q_eval, y_j)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # soft update target network\n",
    "        self.softUpdate()\n",
    "\n",
    "    def softUpdate(self):\n",
    "        for eval_param, target_param in zip(self.net_eval.parameters(), self.net_target.parameters()):\n",
    "            target_param.data.copy_(self.tau*eval_param.data + (1.0-self.tau)*target_param.data)\n",
    "\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, n_actions, memory_size, batch_size):\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen = memory_size)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1e3e9",
   "metadata": {},
   "source": [
    "Traning and Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40bd905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent, n_episodes=2000, max_steps=1000, eps_start=1.0, eps_end=0.08, eps_decay=0.995, target=250, chkpt=False):\n",
    "    score_hist = []\n",
    "    epsilon = eps_start\n",
    "\n",
    "    bar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt} [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
    "    # bar_format = '{l_bar}{bar:10}{r_bar}'\n",
    "    pbar = trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
    "    for idx_epi in pbar:\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        for idx_step in range(max_steps):\n",
    "            action = agent.getAction(state, epsilon)\n",
    "            next_state, reward, done, _, info = env.step(action)\n",
    "            agent.save2memory(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        score_hist.append(score)\n",
    "        score_avg = np.mean(score_hist[-100:])\n",
    "        epsilon = max(eps_end, epsilon*eps_decay)\n",
    "\n",
    "        pbar.set_postfix_str(f\"Score: {score: 7.2f}, 100 score avg: {score_avg: 7.2f}, epsilon: {epsilon: 7.2f}\")\n",
    "        pbar.update(0)\n",
    "\n",
    "        # if (idx_epi+1) % 100 == 0:\n",
    "        #     print(\" \")\n",
    "        #     sleep(0.1)\n",
    "\n",
    "        # Early stop\n",
    "        if len(score_hist) >= 100:\n",
    "            if score_avg >= target:\n",
    "                break\n",
    "\n",
    "    if (idx_epi+1) < n_episodes:\n",
    "        print(\"\\nTarget Reached!\")\n",
    "    else:\n",
    "        print(\"\\nDone!\")\n",
    "        \n",
    "    if chkpt:\n",
    "        torch.save(agent.net_eval.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "    return score_hist\n",
    "\n",
    "#%% Test Lunar Lander\n",
    "def testLander(env, agent, loop=3):\n",
    "    for i in range(loop):\n",
    "        state, info = env.reset()\n",
    "        for idx_step in range(500):\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            env.render()\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    \n",
    "def plotScore(scores):\n",
    "    plt.figure()\n",
    "    plt.plot(scores)\n",
    "    plt.title(\"Score History\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94829961",
   "metadata": {},
   "source": [
    "## 2. Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a9abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "EPISODES = 10000\n",
    "TARGET_SCORE = 275      # early training stop at avg score of last 100 episodes\n",
    "GAMMA = 0.99            # discount factor\n",
    "MEMORY_SIZE = 10000     # max memory buffer size\n",
    "LEARN_STEP = 10          # how often to learn\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "SAVE_CHKPT = False      # save trained network .pth file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02799e76",
   "metadata": {},
   "source": [
    "## 3. Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407052f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "  0%|          |    0/10000 [  00:00<      ?, ?ep/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (33600x3 and 210x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m      3\u001b[0m num_actions \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn\n\u001b[1;32m      4\u001b[0m agent \u001b[39m=\u001b[39m DQN(\n\u001b[1;32m      5\u001b[0m     n_states \u001b[39m=\u001b[39m num_states,\n\u001b[1;32m      6\u001b[0m     n_actions \u001b[39m=\u001b[39m num_actions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     tau \u001b[39m=\u001b[39m TAU,\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0m score_hist \u001b[39m=\u001b[39m train(env, agent, n_episodes\u001b[39m=\u001b[39;49mEPISODES, target\u001b[39m=\u001b[39;49mTARGET_SCORE, chkpt\u001b[39m=\u001b[39;49mSAVE_CHKPT)\n\u001b[1;32m     15\u001b[0m plotScore(score_hist)\n\u001b[1;32m     17\u001b[0m \u001b[39m# if str(device) == \"cuda\":\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m#     torch.cuda.empty_cache()\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, agent, n_episodes, max_steps, eps_start, eps_end, eps_decay, target, chkpt)\u001b[0m\n\u001b[1;32m     10\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m idx_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_steps):\n\u001b[0;32m---> 12\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mgetAction(state, epsilon)\n\u001b[1;32m     13\u001b[0m     next_state, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     14\u001b[0m     agent\u001b[39m.\u001b[39msave2memory(state, action, reward, next_state, done)\n",
      "Cell \u001b[0;32mIn[1], line 64\u001b[0m, in \u001b[0;36mDQN.getAction\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_eval\u001b[39m.\u001b[39meval()\n\u001b[1;32m     63\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 64\u001b[0m     action_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet_eval(state)\n\u001b[1;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_eval\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     67\u001b[0m \u001b[39m# epsilon-greedy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 37\u001b[0m, in \u001b[0;36mQNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (33600x3 and 210x64)"
     ]
    }
   ],
   "source": [
    "env = gym.make('ALE/Breakout-v5')\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "agent = DQN(\n",
    "    n_states = num_states,\n",
    "    n_actions = num_actions,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    lr = LR,\n",
    "    gamma = GAMMA,\n",
    "    mem_size = MEMORY_SIZE,\n",
    "    learn_step = LEARN_STEP,\n",
    "    tau = TAU,\n",
    "    )\n",
    "score_hist = train(env, agent, n_episodes=EPISODES, target=TARGET_SCORE, chkpt=SAVE_CHKPT)\n",
    "plotScore(score_hist)\n",
    "\n",
    "# if str(device) == \"cuda\":\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "if str(device) == \"mps\":\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb59b1",
   "metadata": {},
   "source": [
    "## 4. Test the LunarLander!\n",
    "Run code below to test trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72b48127",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "testLander(env, agent, loop=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4be6b2",
   "metadata": {},
   "source": [
    "Run code below to save tested results into a gif!   \n",
    "Saved gifs will appear in `gifs` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c03247c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving gif...Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import imageio\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def TextOnImg(img, score):\n",
    "    img = Image.fromarray(img)\n",
    "    # font = ImageFont.truetype('/Library/Fonts/arial.ttf', 18)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    # draw.text((20, 20), f\"Score={score: .2f}\", font=font, fill=(255, 255, 255))\n",
    "    draw.text((20, 20), f\"Score={score: .2f}\", fill=(255, 255, 255))\n",
    "\n",
    "    return np.array(img)\n",
    "\n",
    "def save_frames_as_gif(frames, filename, path=\"gifs/\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    print(\"Saving gif...\", end=\"\")\n",
    "    imageio.mimsave(path + filename + \".gif\", frames, duration=30)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "def gym2gif(env, agent, filename=\"gym_animation\", loop=3):\n",
    "    frames = []\n",
    "    for i in range(loop):\n",
    "        state, info = env.reset()\n",
    "        score = 0\n",
    "        for idx_step in range(500):\n",
    "            frame = env.render()\n",
    "            frames.append(TextOnImg(frame, score))\n",
    "            action = agent.getAction(state, epsilon=0)\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "    env.close()\n",
    "    save_frames_as_gif(frames, filename=filename)\n",
    "\n",
    "gym2gif(env, agent, loop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568d402f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "01c36111a1525a5ff407868170e7863364c687246ea4e433c1c4d9e429760133"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
